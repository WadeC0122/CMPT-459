{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First Version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMPT459 2020 spring\n",
    "# milestone2\n",
    "# data cleaned and feature selection and numericalization for classifier\n",
    "# group name: Salt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "%matplotlib inline\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import csv as csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def outlier_fd(data, threshold=0.5):\n",
    "    low, high = np.percentile(data,1), np.percentile(data, 99)\n",
    "    dist = high - low\n",
    "    cut_off = dist * threshold\n",
    "    lower, upper = low-cut_off, high+cut_off\n",
    "    return [lower, upper]\n",
    "\n",
    "def photos_number_counter(x):\n",
    "    return len(x)\n",
    "\n",
    "def valid_des(s):\n",
    "    valid_s = \" \"\n",
    "    for i in s.split():\n",
    "        if i.isalnum():\n",
    "            valid_s = \" \".join([valid_s, i])\n",
    "            \n",
    "    return valid_s.strip()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('train.json')\n",
    "test = pd.read_json('test.json')\n",
    "ptrain = train\n",
    "ptest = test\n",
    "ptrain = ptrain.drop(['interest_level'], axis = 1)\n",
    "pwhole = ptrain.append(ptest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepocessing for training data and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lewislin/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Find the outlier of price, latitude and longitude using bound given by outlier_fd\n",
    "[price_lower, price_higher] = outlier_fd(ptrain['price'])\n",
    "[latitude_lower, latitude_higher] = outlier_fd(ptrain['latitude'],1)\n",
    "[longitude_lower, longitude_higher] = outlier_fd(ptrain['longitude'],1)\n",
    "#deal with outliers, set the column for modified data and plot again\n",
    "pwhole['price_modify'] = pwhole['price']\n",
    "pwhole['price_modify'].loc[pwhole['price_modify']<price_lower] = price_lower\n",
    "pwhole['price_modify'].loc[pwhole['price_modify']>price_higher] = price_higher\n",
    "#deal with outliers, set the column for modified data and plot again\n",
    "pwhole['latitude_modify'] = pwhole['latitude']\n",
    "pwhole['latitude_modify'].loc[pwhole['latitude_modify']<latitude_lower] = latitude_lower\n",
    "pwhole['latitude_modify'].loc[pwhole['latitude_modify']>latitude_higher] = latitude_higher\n",
    "#deal with outliers, set the column for modified data and plot again\n",
    "pwhole['longitude_modify'] = pwhole['longitude']\n",
    "pwhole['longitude_modify'].loc[pwhole['longitude']<longitude_lower] = longitude_lower\n",
    "pwhole['longitude_modify'].loc[pwhole['longitude']>longitude_higher] = longitude_higher\n",
    "#get the number of photos\n",
    "pwhole['photos_number'] = pwhole['photos'].apply(photos_number_counter)\n",
    "#set all feature words in tf_idf as features for train for feature_modify\n",
    "pwhole['features_modify'] = pwhole['features'].apply(lambda x:\" \".join([\"_\".join(i.split(\" \"))for i in x]))\n",
    "counterf = CountVectorizer(max_features=100)\n",
    "train_sparse_f = counterf.fit_transform(pwhole['features_modify'])\n",
    "feature_list = counterf.get_feature_names()\n",
    "array1 = train_sparse_f.toarray()\n",
    "for i, feature_str in enumerate(feature_list):\n",
    "    list_all = []\n",
    "    [rows, cols] = array1.shape\n",
    "    for row in range(rows):\n",
    "        list_all.append(array1[row][i])\n",
    "    feature_str_ = feature_str + '(f)'\n",
    "    feature_v = pd.Series(list_all,pwhole.index, name = feature_str_)\n",
    "    pwhole[feature_str_] = feature_v\n",
    "#set all feature words in tf_idf as features for train for description_modify\n",
    "pwhole['description_modify'] = pwhole['description'].apply(lambda x:valid_des(x))\n",
    "counterd = CountVectorizer(stop_words=['features'], max_features=100)\n",
    "train_sparse_d = counterd.fit_transform(pwhole['description_modify'])\n",
    "description_list = counterd.get_feature_names()\n",
    "array2 = train_sparse_d.toarray()\n",
    "for i, description_str in enumerate(description_list):\n",
    "    list_all = []\n",
    "    [rows, cols] = array2.shape\n",
    "    for row in range(rows):\n",
    "        list_all.append(array2[row][i])\n",
    "    description_str_ = description_str + '(d)'\n",
    "    description_v = pd.Series(list_all,pwhole.index, name = description_str_)\n",
    "    pwhole[description_str_] = description_v\n",
    "## /*add something to change the created to year month day*/\n",
    "pwhole['created']=pd.to_datetime(pwhole[\"created\"])\n",
    "pwhole['year']=pwhole['created'].dt.year\n",
    "pwhole['month']=pwhole['created'].dt.month\n",
    "pwhole['day']=pwhole['created'].dt.day\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwhole = pwhole.drop(['central_a(f)','in_super(f)','on(f)','pre(f)','war(f)'], axis = 1)\n",
    "pwhole = pwhole.drop(['an(d)','and(d)','are(d)','as(d)','at(d)','be(d)','by(d)','can(d)',\n",
    "                      'for(d)','from(d)','in(d)','is(d)','it(d)','just(d)','me(d)','more(d)',\n",
    "                      'natural(d)','new(d)','or(d)','park(d)','that(d)','the(d)','this(d)',\n",
    "                      'to(d)','will(d)','with(d)','you(d)','your(d)'], axis = 1)\n",
    "pwhole = pwhole.drop([\"building_id\", \"created\", \"description\", \"display_address\", \"features\", \n",
    "                     \"manager_id\", \"photos\", \"street_address\", \"price\", \"longitude\",\n",
    "                        \"latitude\", \"features_modify\", \"description_modify\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pwhole[:49352]\n",
    "X_Test = pwhole[49352:]\n",
    "X = X.drop(['listing_id'], axis = 1)\n",
    "X_Test = X_Test.drop(['listing_id'], axis = 1)\n",
    "Y = pd.get_dummies(train['interest_level'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have three datasets:\n",
    "\n",
    "training samples(without labels) -- X\n",
    "\n",
    "training labels -- Y\n",
    "\n",
    "test dataset -- X_Test\n",
    "\n",
    "\n",
    "the columns of X_train is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>medium</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124002</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124004</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124008</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124009</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49352 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        high  low  medium\n",
       "4          0    0       1\n",
       "6          0    1       0\n",
       "9          0    0       1\n",
       "10         0    0       1\n",
       "15         0    1       0\n",
       "...      ...  ...     ...\n",
       "124000     0    1       0\n",
       "124002     0    0       1\n",
       "124004     0    0       1\n",
       "124008     0    0       1\n",
       "124009     1    0       0\n",
       "\n",
       "[49352 rows x 3 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(0,49352):\n",
    "#     if Y.loc[i,'high']==1:\n",
    "#         Y.loc[i, 'target']=0\n",
    "#     if Y.loc[i,'low']==1:\n",
    "#         Y.loc[i, 'target']=1\n",
    "#     if Y.loc[i,'medium']==1:\n",
    "#         Y.loc[i, 'target']=2\n",
    "# Y=Y.drop(['high','low','medium'], axis = 1)\n",
    "# Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-fold Cross-validation (repeats 5 times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use 5-fold cross-validation here and choose the classifer with lowest log loss(Best_OVR1), highest classification accuracy(Best_OVR2), highest precision(Best_OVR3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training log_loss: 0.563\n",
      "validation log_loss: 0.608\n",
      "training log_loss: 0.565\n",
      "validation log_loss: 0.599\n",
      "training log_loss: 0.565\n",
      "validation log_loss: 0.598\n",
      "training log_loss: 0.561\n",
      "validation log_loss: 0.613\n",
      "training log_loss: 0.563\n",
      "validation log_loss: 0.607\n",
      "average validation log_loss: 0.605\n",
      "training log_loss: 0.563\n",
      "validation log_loss: 0.605\n",
      "training log_loss: 0.567\n",
      "validation log_loss: 0.591\n",
      "training log_loss: 0.559\n",
      "validation log_loss: 0.621\n",
      "training log_loss: 0.562\n",
      "validation log_loss: 0.612\n",
      "training log_loss: 0.565\n",
      "validation log_loss: 0.596\n",
      "average validation log_loss: 0.605\n",
      "training log_loss: 0.563\n",
      "validation log_loss: 0.606\n",
      "training log_loss: 0.561\n",
      "validation log_loss: 0.614\n",
      "training log_loss: 0.565\n",
      "validation log_loss: 0.597\n",
      "training log_loss: 0.563\n",
      "validation log_loss: 0.606\n",
      "training log_loss: 0.563\n",
      "validation log_loss: 0.604\n",
      "average validation log_loss: 0.605\n",
      "training log_loss: 0.565\n",
      "validation log_loss: 0.598\n",
      "training log_loss: 0.563\n",
      "validation log_loss: 0.606\n",
      "training log_loss: 0.562\n",
      "validation log_loss: 0.608\n",
      "training log_loss: 0.564\n",
      "validation log_loss: 0.601\n",
      "training log_loss: 0.562\n",
      "validation log_loss: 0.609\n",
      "average validation log_loss: 0.604\n",
      "training log_loss: 0.562\n",
      "validation log_loss: 0.612\n",
      "training log_loss: 0.563\n",
      "validation log_loss: 0.604\n",
      "training log_loss: 0.564\n",
      "validation log_loss: 0.603\n",
      "training log_loss: 0.562\n",
      "validation log_loss: 0.604\n",
      "training log_loss: 0.564\n",
      "validation log_loss: 0.603\n",
      "average validation log_loss: 0.605\n",
      "One vs rest best validation accuracy: 0.591\n",
      "[0.6429946307365009, 0.651605713706818, 0.6422492401215806, 0.6383991894630192, 0.6447821681864235, 0.6469455982169993, 0.6527200891500354, 0.6367781155015197, 0.6377912867274569, 0.6488348530901722, 0.6357005369263499, 0.642386789585655, 0.648936170212766, 0.6420466058763931, 0.6446808510638298, 0.6489717353864857, 0.6410698004254888, 0.6455927051671733, 0.6491388044579534, 0.6382978723404256, 0.6393475838314254, 0.6442103130381927, 0.646403242147923, 0.6412360688956433, 0.6451874366767983]\n"
     ]
    }
   ],
   "source": [
    "training_score = 0\n",
    "validation_score = 0\n",
    "Best_validation_score = 20\n",
    "count = 0\n",
    "total_validation_score = 0\n",
    "precision = []\n",
    "recall = []\n",
    "test_accuracy =[]\n",
    "test_accuracy_value = 0\n",
    "test_precision = 0\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# scrores\n",
    "#maybe 5-fold is better than 10-fold\n",
    "kf = RepeatedKFold(n_splits=5, n_repeats=5, random_state=None) \n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "#     print(\"Train:\", train_index, \"Validation:\",test_index)\n",
    "    X['id']=list(range(0,49352))\n",
    "    X.set_index(\"id\", inplace=True)\n",
    "    Y['id']=list(range(0,49352))\n",
    "    Y.set_index(\"id\", inplace=True)\n",
    "    X_train, X_test = X.loc[train_index,:], X.loc[test_index,:]\n",
    "    Y_train, Y_test = Y.loc[train_index,:], Y.loc[test_index,:]\n",
    "#     OVR = OneVsRestClassifier(LogisticRegression(warm_start=True,max_iter=200,tol=0.00001)).fit(X_train,Y_train)\n",
    "#     ABC = AdaBoostClassifier(n_estimators=150,base_estimator=LogisticRegression(warm_start=True,max_iter=200,tol=0.00001))\n",
    "#     rfc = RandomForestClassifier()\n",
    "#     rfc.fit(X_train,Y_train)\n",
    "#     ABC = AdaBoostClassifier(n_estimators=5,base_estimator=LogisticRegression(warm_start=True,max_iter=200,tol=0.00001))\n",
    "#     ABC = OneVsRestClassifier(AdaBoostClassifier(n_estimators=50,base_estimator=DecisionTreeClassifier(max_depth=6)))\n",
    "\n",
    "#     ABC = OneVsRestClassifier(GradientBoostingClassifier(n_estimators=150))\n",
    "    ABC = OneVsRestClassifier(HistGradientBoostingClassifier(max_iter=50, tol=1e-07))\n",
    "    ABC = ABC.fit(X_train,Y_train)\n",
    "#   classification accuracy metrics\n",
    "    if test_accuracy_value<ABC.score(X_test,Y_test):\n",
    "        test_accuracy_value=ABC.score(X_test,Y_test)\n",
    "        Best_ABC2 = ABC\n",
    "    test_accuracy.append(ABC.score(X_test,Y_test))\n",
    "#   log loss on training dataset\n",
    "    Y_Prediction=ABC.predict_proba(X_train)\n",
    "    Y_Prediction=pd.DataFrame(Y_Prediction)\n",
    "    Y_train=pd.DataFrame(Y_train)\n",
    "    training_score=log_loss(Y_train, Y_Prediction)\n",
    "    print(\"training log_loss: %0.3f\" % training_score)\n",
    "\n",
    "#   log loss on test dataset\n",
    "    Y_Prediction=ABC.predict_proba(X_test)\n",
    "    Y_Prediction=pd.DataFrame(Y_Prediction)\n",
    "    Y_test=pd.DataFrame(Y_test)\n",
    "    validation_score=log_loss(Y_test, Y_Prediction)\n",
    "    count+=1\n",
    "    total_validation_score+=validation_score\n",
    "    print(\"validation log_loss: %0.3f\" % validation_score)\n",
    "\n",
    "#   logloss metrics\n",
    "    if validation_score<Best_validation_score:\n",
    "        Best_validation_score = validation_score\n",
    "        Best_ABC1 = ABC\n",
    "\n",
    "    if count==5:\n",
    "        average = total_validation_score/float(count)\n",
    "        print(\"average validation log_loss: %0.3f\" % average)\n",
    "        count=0\n",
    "        total_validation_score=0\n",
    "    \n",
    "#     Y_Prediction=OVR.predict(X_test)\n",
    "# #   print confusion matrix\n",
    "#     print(multilabel_confusion_matrix(Y_test, Y_Prediction))\n",
    "# #   confusion matrix matrics\n",
    "#     if test_precision<precision_score(Y_test, Y_Prediction, average='macro'):\n",
    "#         test_precision=precision_score(Y_test, Y_Prediction, average='macro')\n",
    "#         Best_OVR3 = OVR\n",
    "#     precision.append(precision_score(Y_test, Y_Prediction, average='macro'))\n",
    "#     recall.append(recall_score(Y_test, Y_Prediction, average='macro'))\n",
    "print (\"One vs rest best validation accuracy: %0.3f\" % Best_validation_score) \n",
    "# print(precision)\n",
    "# print(recall)\n",
    "print (test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write into submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I use the best classifier evaluated by log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Prediction=Best_ABC1.predict_proba(X_Test)  #use the best classifier evaluated by log loss\n",
    "y=pd.DataFrame(Y_Prediction)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "ids = test['listing_id']\n",
    "ids.reset_index(drop=True, inplace=True)\n",
    "out = pd.concat([ids, y],axis=1, ignore_index=True)\n",
    "out\n",
    "# out.columns=['listing_id','high','medium','low']\n",
    "out.columns=['listing_id','high','low','medium']\n",
    "submission_file = open(\"submission.csv\", \"w\")\n",
    "out.to_csv('submission.csv', index = False)\n",
    "submission_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is mainly first version and I did some feature selections corresponding to the modification part(a) in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMPT459 2020 spring\n",
    "# milestone2\n",
    "# data cleaned and feature selection and numericalization for classifier\n",
    "# group name: Salt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os\n",
    "%matplotlib inline\n",
    "from sklearn import model_selection, preprocessing, ensemble\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import csv as csv\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def outlier_fd(data, threshold=0.5):\n",
    "    low, high = np.percentile(data,1), np.percentile(data, 99)\n",
    "    dist = high - low\n",
    "    cut_off = dist * threshold\n",
    "    lower, upper = low-cut_off, high+cut_off\n",
    "    return [lower, upper]\n",
    "\n",
    "def photos_number_counter(x):\n",
    "    return len(x)\n",
    "\n",
    "def valid_des(s):\n",
    "    valid_s = \" \"\n",
    "    for i in s.split():\n",
    "        if i.isalnum():\n",
    "            valid_s = \" \".join([valid_s, i])\n",
    "            \n",
    "    return valid_s.strip()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_json('train.json')\n",
    "test = pd.read_json('test.json')\n",
    "ptrain = train\n",
    "ptest = test\n",
    "ptrain = ptrain.drop(['interest_level'], axis = 1)\n",
    "pwhole = ptrain.append(ptest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lewislin/opt/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:205: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_with_indexer(indexer, value)\n"
     ]
    }
   ],
   "source": [
    "# Find the outlier of price, latitude and longitude using bound given by outlier_fd\n",
    "[price_lower, price_higher] = outlier_fd(ptrain['price'])\n",
    "[latitude_lower, latitude_higher] = outlier_fd(ptrain['latitude'],1)\n",
    "[longitude_lower, longitude_higher] = outlier_fd(ptrain['longitude'],1)\n",
    "#deal with outliers, set the column for modified data and plot again\n",
    "pwhole['price_modify'] = pwhole['price']\n",
    "pwhole['price_modify'].loc[pwhole['price_modify']<price_lower] = price_lower\n",
    "pwhole['price_modify'].loc[pwhole['price_modify']>price_higher] = price_higher\n",
    "#deal with outliers, set the column for modified data and plot again\n",
    "pwhole['latitude_modify'] = pwhole['latitude']\n",
    "pwhole['latitude_modify'].loc[pwhole['latitude_modify']<latitude_lower] = latitude_lower\n",
    "pwhole['latitude_modify'].loc[pwhole['latitude_modify']>latitude_higher] = latitude_higher\n",
    "#deal with outliers, set the column for modified data and plot again\n",
    "pwhole['longitude_modify'] = pwhole['longitude']\n",
    "pwhole['longitude_modify'].loc[pwhole['longitude']<longitude_lower] = longitude_lower\n",
    "pwhole['longitude_modify'].loc[pwhole['longitude']>longitude_higher] = longitude_higher\n",
    "#get the number of photos\n",
    "pwhole['photos_number'] = pwhole['photos'].apply(photos_number_counter)\n",
    "#set all feature words in tf_idf as features for train for feature_modify\n",
    "pwhole['features_modify'] = pwhole['features'].apply(lambda x:\" \".join([\"_\".join(i.split(\" \"))for i in x]))\n",
    "counterf = CountVectorizer(max_features=100)\n",
    "train_sparse_f = counterf.fit_transform(pwhole['features_modify'])\n",
    "feature_list = counterf.get_feature_names()\n",
    "array1 = train_sparse_f.toarray()\n",
    "for i, feature_str in enumerate(feature_list):\n",
    "    list_all = []\n",
    "    [rows, cols] = array1.shape\n",
    "    for row in range(rows):\n",
    "        list_all.append(array1[row][i])\n",
    "    feature_str_ = feature_str + '(f)'\n",
    "    feature_v = pd.Series(list_all,pwhole.index, name = feature_str_)\n",
    "    pwhole[feature_str_] = feature_v\n",
    "#set all feature words in tf_idf as features for train for description_modify\n",
    "pwhole['description_modify'] = pwhole['description'].apply(lambda x:valid_des(x))\n",
    "counterd = CountVectorizer(stop_words=['features'], max_features=100)\n",
    "train_sparse_d = counterd.fit_transform(pwhole['description_modify'])\n",
    "description_list = counterd.get_feature_names()\n",
    "array2 = train_sparse_d.toarray()\n",
    "for i, description_str in enumerate(description_list):\n",
    "    list_all = []\n",
    "    [rows, cols] = array2.shape\n",
    "    for row in range(rows):\n",
    "        list_all.append(array2[row][i])\n",
    "    description_str_ = description_str + '(d)'\n",
    "    description_v = pd.Series(list_all,pwhole.index, name = description_str_)\n",
    "    pwhole[description_str_] = description_v\n",
    "## /*add something to change the created to year month day*/\n",
    "pwhole['created']=pd.to_datetime(pwhole[\"created\"])\n",
    "pwhole['year']=pwhole['created'].dt.year\n",
    "pwhole['month']=pwhole['created'].dt.month\n",
    "pwhole['day']=pwhole['created'].dt.day\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pwhole = pwhole.drop(['central_a(f)','in_super(f)','on(f)','pre(f)','war(f)'], axis = 1)\n",
    "pwhole = pwhole.drop(['an(d)','and(d)','are(d)','as(d)','at(d)','be(d)','by(d)','can(d)',\n",
    "                      'for(d)','from(d)','in(d)','is(d)','it(d)','just(d)','me(d)','more(d)',\n",
    "                      'natural(d)','new(d)','or(d)','park(d)','that(d)','the(d)','this(d)',\n",
    "                      'to(d)','will(d)','with(d)','you(d)','your(d)'], axis = 1)\n",
    "pwhole = pwhole.drop([\"building_id\", \"created\", \"description\", \"display_address\", \"features\", \n",
    "                     \"manager_id\", \"photos\", \"street_address\", \"price\", \"longitude\",\n",
    "                        \"latitude\", \"features_modify\", \"description_modify\"], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pwhole[:49352]\n",
    "X_Test = pwhole[49352:]\n",
    "X = X.drop(['listing_id'], axis = 1)\n",
    "X_Test = X_Test.drop(['listing_id'], axis = 1)\n",
    "Y = pd.get_dummies(train['interest_level'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lewislin/opt/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([9.82902159e-03, 2.35845076e-02, 5.30274181e-02, 2.96596259e-02,\n",
       "       2.90466952e-02, 2.91141293e-02, 4.32790683e-04, 5.13846006e-04,\n",
       "       3.30924876e-03, 1.23402200e-04, 6.36228429e-03, 5.40885101e-05,\n",
       "       9.90911154e-05, 2.62655144e-03, 1.28285735e-04, 1.30855560e-04,\n",
       "       1.97011745e-04, 5.13207432e-04, 4.71072767e-03, 7.61638335e-03,\n",
       "       6.35055291e-03, 9.28850620e-03, 3.80912531e-04, 3.62260892e-04,\n",
       "       2.36093155e-04, 1.54924129e-04, 8.50298219e-03, 4.37500326e-03,\n",
       "       3.87451902e-04, 1.79251156e-03, 5.59023895e-05, 5.81537908e-03,\n",
       "       4.08216663e-05, 3.14992338e-03, 7.78208351e-04, 2.34443470e-03,\n",
       "       2.40457311e-04, 3.57621347e-04, 2.18632656e-04, 5.53391421e-04,\n",
       "       1.40971658e-02, 4.97433845e-04, 8.26995140e-04, 3.27722169e-03,\n",
       "       3.51688217e-05, 3.47158731e-04, 2.70175730e-04, 1.08309055e-02,\n",
       "       7.25225888e-03, 8.27835850e-04, 9.13418984e-04, 6.42651600e-04,\n",
       "       4.46281247e-04, 4.59444575e-04, 3.43506192e-03, 8.31765532e-05,\n",
       "       4.78338874e-04, 6.40496273e-05, 3.26287914e-04, 1.00451715e-03,\n",
       "       2.77689064e-03, 7.37883024e-04, 1.22434157e-02, 4.23803042e-04,\n",
       "       4.94486403e-05, 8.63001447e-05, 4.28095490e-03, 1.94187688e-04,\n",
       "       1.20983247e-03, 2.47045931e-03, 6.10739002e-04, 2.45605835e-04,\n",
       "       1.31753710e-03, 1.45359232e-04, 2.66409355e-03, 2.70117862e-04,\n",
       "       5.12622451e-03, 1.05171805e-03, 1.33299237e-05, 2.55770947e-04,\n",
       "       3.10426730e-04, 4.20086691e-03, 9.30928104e-05, 5.95607924e-04,\n",
       "       7.74980230e-04, 2.91164943e-04, 5.51392913e-04, 1.34592738e-04,\n",
       "       4.82796033e-04, 3.55606442e-04, 2.47961193e-04, 2.91134402e-03,\n",
       "       2.59889505e-03, 1.35130614e-04, 1.13803592e-04, 6.58271080e-05,\n",
       "       2.83130689e-04, 3.08066388e-04, 2.83063045e-04, 1.80758709e-03,\n",
       "       5.98273857e-05, 6.10834642e-03, 9.62478822e-03, 7.25560713e-03,\n",
       "       1.38014769e-02, 4.77448862e-03, 7.18653741e-03, 7.06525093e-03,\n",
       "       7.34013993e-03, 8.71010870e-03, 1.24700661e-02, 4.79595255e-03,\n",
       "       1.02526433e-02, 1.01598215e-02, 6.43794075e-03, 5.38653039e-03,\n",
       "       7.63931737e-03, 8.23093657e-03, 8.85607804e-03, 4.83382948e-03,\n",
       "       8.55797032e-03, 8.27353036e-03, 2.59109235e-03, 4.04896489e-03,\n",
       "       4.66928949e-03, 6.63000651e-03, 6.94066130e-03, 6.26811402e-03,\n",
       "       8.56170188e-03, 7.42095825e-03, 1.04928252e-02, 9.67257896e-03,\n",
       "       8.61704566e-03, 6.59908735e-03, 6.65810050e-03, 9.19220896e-03,\n",
       "       2.01327340e-03, 5.48751273e-03, 4.99905037e-03, 1.07369758e-02,\n",
       "       1.08238983e-02, 8.49271745e-03, 9.09722096e-03, 9.97143141e-03,\n",
       "       5.14886921e-03, 7.53703936e-03, 5.84330699e-03, 8.68825493e-03,\n",
       "       1.34656769e-02, 1.19460942e-02, 8.79362563e-03, 5.83888258e-03,\n",
       "       6.17337564e-03, 7.69863307e-03, 4.40649341e-03, 1.05418525e-02,\n",
       "       8.02612845e-03, 6.71986604e-03, 7.36934995e-03, 7.31301234e-03,\n",
       "       8.50125173e-03, 8.31982240e-03, 8.75159749e-03, 7.01297181e-03,\n",
       "       7.87607483e-03, 6.92914063e-03, 1.87993152e-03, 7.30108340e-03,\n",
       "       6.97418181e-03, 7.93956308e-03, 6.73841577e-03, 7.01256736e-03,\n",
       "       4.31933262e-03, 0.00000000e+00, 3.97705976e-02, 7.20574030e-02])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "fsclf = ExtraTreesClassifier()\n",
    "fsclf = fsclf.fit(X, Y)\n",
    "fsclf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bathrooms', 'bedrooms', 'price_modify', 'latitude_modify',\n",
       "       'longitude_modify', 'photos_number', '_photos(f)', 'actual_apt(f)',\n",
       "       'balcony(f)', 'bike_room(f)',\n",
       "       ...\n",
       "       'supports(d)', 'text(d)', 'two(d)', 'unit(d)', 'west(d)', 'windows(d)',\n",
       "       'york(d)', 'year', 'month', 'day'],\n",
       "      dtype='object', length=176)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = X.columns\n",
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.07205740299664692, 'day'),\n",
       " (0.05302741805273241, 'price_modify'),\n",
       " (0.039770597568169606, 'month'),\n",
       " (0.029659625894016356, 'latitude_modify'),\n",
       " (0.029114129335232948, 'photos_number'),\n",
       " (0.029046695247906568, 'longitude_modify'),\n",
       " (0.023584507618324705, 'bedrooms'),\n",
       " (0.01409716582245891, 'hardwood_floors(f)'),\n",
       " (0.013801476917525191, 'apartment(d)'),\n",
       " (0.013465676884601582, 'of(d)'),\n",
       " (0.012470066082610245, 'bedroom(d)'),\n",
       " (0.012243415726651764, 'no_fee(f)'),\n",
       " (0.011946094162706573, 'on(d)'),\n",
       " (0.010830905534187226, 'laundry_in_building(f)'),\n",
       " (0.010823898278304604, 'large(d)'),\n",
       " (0.010736975760043491, 'kitchen(d)'),\n",
       " (0.010541852501813411, 'renovated(d)'),\n",
       " (0.010492825201911487, 'great(d)'),\n",
       " (0.010252643288963508, 'building(d)'),\n",
       " (0.010159821464455153, 'call(d)'),\n",
       " (0.00997143141068902, 'located(d)'),\n",
       " (0.009829021591743469, 'bathrooms'),\n",
       " (0.009672578963100044, 'hardwood(d)'),\n",
       " (0.009624788215813326, 'all(d)'),\n",
       " (0.009288506199483227, 'doorman(f)'),\n",
       " (0.009192208964599371, 'high(d)'),\n",
       " (0.00909722095743148, 'living(d)'),\n",
       " (0.008856078041433136, 'contact(d)'),\n",
       " (0.008793625625012413, 'one(d)'),\n",
       " (0.008751597487020766, 'steel(d)'),\n",
       " (0.00871010869837376, 'beautiful(d)'),\n",
       " (0.008688254927520006, 'no(d)'),\n",
       " (0.00861704565752829, 'has(d)'),\n",
       " (0.008561701881574995, 'full(d)'),\n",
       " (0.008557970322820723, 'east(d)'),\n",
       " (0.008502982187612465, 'elevator(f)'),\n",
       " (0.008501251727402456, 'spacious(d)'),\n",
       " (0.008492717450049873, 'laundry(d)'),\n",
       " (0.008319822402329636, 'stainless(d)'),\n",
       " (0.008273530359290761, 'email(d)'),\n",
       " (0.008230936570020095, 'closet(d)'),\n",
       " (0.008026128452424992, 'room(d)'),\n",
       " (0.00793956308150904, 'unit(d)'),\n",
       " (0.007876074827893655, 'studio(d)'),\n",
       " (0.00769863306776559, 'private(d)'),\n",
       " (0.007639317372887179, 'close(d)'),\n",
       " (0.007616383346265583, 'dishwasher(f)'),\n",
       " (0.007537039364176589, 'marble(d)'),\n",
       " (0.007420958253938535, 'granite(d)'),\n",
       " (0.007369349950851975, 'size(d)'),\n",
       " (0.007340139933172886, 'bathroom(d)'),\n",
       " (0.007313012340679527, 'space(d)'),\n",
       " (0.007301083397556821, 'text(d)'),\n",
       " (0.007255607126861065, 'amazing(d)'),\n",
       " (0.007252258882530327, 'laundry_in_unit(f)'),\n",
       " (0.007186537409603762, 'appliances(d)'),\n",
       " (0.007065250926940487, 'available(d)'),\n",
       " (0.007012971807550826, 'steps(d)'),\n",
       " (0.0070125673552820554, 'windows(d)'),\n",
       " (0.006974181812212125, 'two(d)'),\n",
       " (0.006940661304439416, 'floors(d)'),\n",
       " (0.006929140631052306, 'subway(d)'),\n",
       " (0.006738415766809461, 'west(d)'),\n",
       " (0.0067198660398050504, 'schedule(d)'),\n",
       " (0.006658100497489221, 'heart(d)'),\n",
       " (0.006630006512003707, 'floor(d)'),\n",
       " (0.006599087351164502, 'have(d)'),\n",
       " (0.006437940745574132, 'central(d)'),\n",
       " (0.0063622842934964435, 'cats_allowed(f)'),\n",
       " (0.0063505529088908426, 'dogs_allowed(f)'),\n",
       " (0.006268114022344234, 'free(d)'),\n",
       " (0.006173375641301351, 'please(d)'),\n",
       " (0.00610834641833181, 'access(d)'),\n",
       " (0.005843306990512982, 'modern(d)'),\n",
       " (0.005838882575800437, 'open(d)'),\n",
       " (0.0058153790765118245, 'fitness_center(f)'),\n",
       " (0.0054875127333568205, 'including(d)'),\n",
       " (0.005386530389468503, 'city(d)'),\n",
       " (0.005148869209959571, 'luxury(d)'),\n",
       " (0.005126224505181781, 'reduced_fee(f)'),\n",
       " (0.00499905036965907, 'information(d)'),\n",
       " (0.004833829479210361, 'doorman(d)'),\n",
       " (0.004795952552662192, 'broker(d)'),\n",
       " (0.004774488620749006, 'apartments(d)'),\n",
       " (0.0047107276673927605, 'dining_room(f)'),\n",
       " (0.004669289493916371, 'fitness(d)'),\n",
       " (0.004406493413214214, 'real(d)'),\n",
       " (0.004375003256149633, 'exclusive(f)'),\n",
       " (0.004319332622691562, 'york(d)'),\n",
       " (0.004280954904125007, 'outdoor_space(f)'),\n",
       " (0.00420086691305372, 'roof_deck(f)'),\n",
       " (0.004048964890738887, 'estate(d)'),\n",
       " (0.003435061919180028, 'loft(f)'),\n",
       " (0.0033092487618725636, 'balcony(f)'),\n",
       " (0.003277221689009174, 'high_speed_internet(f)'),\n",
       " (0.0031499233802018745, 'furnished(f)'),\n",
       " (0.002911344018422283, 'swimming_pool(f)'),\n",
       " (0.002776890635221918, 'new_construction(f)'),\n",
       " (0.002664093551538641, 'private_outdoor_space(f)'),\n",
       " (0.0026265514449160337, 'common_outdoor_space(f)'),\n",
       " (0.0025988950544044197, 'terrace(f)'),\n",
       " (0.0025910923522256216, 'equal(d)'),\n",
       " (0.0024704593096531143, 'patio(f)'),\n",
       " (0.0023444347048089207, 'garden(f)'),\n",
       " (0.0020132734023411425, 'housing(d)'),\n",
       " (0.0018799315175774972, 'supports(d)'),\n",
       " (0.0018075870886820286, 'wheelchair_access(f)'),\n",
       " (0.0017925115561607916, 'fireplace(f)'),\n",
       " (0.0013175370952867055, 'prewar(f)'),\n",
       " (0.0012098324737510196, 'parking_space(f)'),\n",
       " (0.0010517180527033674, 'renovated(f)'),\n",
       " (0.0010045171528930757, 'multi(f)'),\n",
       " (0.0009134189844759256, 'level(f)'),\n",
       " (0.0008278358498723457, 'laundry_room(f)'),\n",
       " (0.0008269951398358722, 'high_ceilings(f)'),\n",
       " (0.0007782083506096625, 'garage(f)'),\n",
       " (0.0007749802298520046, 'simplex(f)'),\n",
       " (0.0007378830243165679, 'newly_renovated(f)'),\n",
       " (0.0006426516002974583, 'light(f)'),\n",
       " (0.0006107390017682777, 'pets_on_approval(f)'),\n",
       " (0.0005956079235816339, 'short_term_allowed(f)'),\n",
       " (0.0005533914214417423, 'hardwood(f)'),\n",
       " (0.0005513929134096969, 'site_laundry(f)'),\n",
       " (0.000513846006266871, 'actual_apt(f)'),\n",
       " (0.0005132074322270072, 'deck(f)'),\n",
       " (0.0004974338449825891, 'high_ceiling(f)'),\n",
       " (0.0004827960334163146, 'stainless_steel_appliances(f)'),\n",
       " (0.00047833887441009937, 'lowrise(f)'),\n",
       " (0.0004594445753511586, 'live_in_super(f)'),\n",
       " (0.00044628124730732585, 'live(f)'),\n",
       " (0.0004327906828470396, '_photos(f)'),\n",
       " (0.00042380304192419973, 'no_pets(f)'),\n",
       " (0.0003874519016978384, 'exposed_brick(f)'),\n",
       " (0.000380912530961878, 'dryer(f)'),\n",
       " (0.00036226089182798016, 'dryer_in_unit(f)'),\n",
       " (0.0003576213469685558, 'green_building(f)'),\n",
       " (0.00035560644233298747, 'storage(f)'),\n",
       " (0.0003471587309010142, 'in_superintendent(f)'),\n",
       " (0.00032628791407709833, 'marble_bath(f)'),\n",
       " (0.0003104267299425737, 'roof(f)'),\n",
       " (0.0003080663880973771, 'washer(f)'),\n",
       " (0.0002911649433061194, 'site_garage(f)'),\n",
       " (0.00028313068911170637, 'walk_in_closet(f)'),\n",
       " (0.00028306304527953745, 'washer_in_unit(f)'),\n",
       " (0.00027017572974436996, 'laundry(f)'),\n",
       " (0.0002701178622066845, 'publicoutdoor(f)'),\n",
       " (0.0002557709472809416, 'residents_lounge(f)'),\n",
       " (0.0002479611934610261, 'subway(f)'),\n",
       " (0.00024560583527173186, 'pool(f)'),\n",
       " (0.00024045731077493377, 'granite_kitchen(f)'),\n",
       " (0.00023609315477786928, 'duplex(f)'),\n",
       " (0.0002186326560881235, 'gym(f)'),\n",
       " (0.00019701174475988885, 'concierge(f)'),\n",
       " (0.0001941876882291879, 'parking(f)'),\n",
       " (0.0001549241289052426, 'eat_in_kitchen(f)'),\n",
       " (0.00014535923236096314, 'private(f)'),\n",
       " (0.00013513061382627119, 'time_doorman(f)'),\n",
       " (0.00013459273769020151, 'space(f)'),\n",
       " (0.00013085555992774816, 'common_roof_deck(f)'),\n",
       " (0.00012828573496475462, 'common_parking(f)'),\n",
       " (0.00012340220031740337, 'bike_room(f)'),\n",
       " (0.00011380359230000855, 'valet(f)'),\n",
       " (9.909111536374917e-05, 'childrens_playroom(f)'),\n",
       " (9.309281041376588e-05, 'roofdeck(f)'),\n",
       " (8.630014474150617e-05, 'outdoor_areas(f)'),\n",
       " (8.317655320161417e-05, 'lounge(f)'),\n",
       " (6.582710798746567e-05, 'view(f)'),\n",
       " (6.404962727511366e-05, 'luxury_building(f)'),\n",
       " (5.9827385749612986e-05, 'wifi_access(f)'),\n",
       " (5.590238945329983e-05, 'fitness(f)'),\n",
       " (5.408851013229844e-05, 'children(f)'),\n",
       " (4.944864029113184e-05, 'outdoor(f)'),\n",
       " (4.0821666299427424e-05, 'full(f)'),\n",
       " (3.516882168869198e-05, 'highrise(f)'),\n",
       " (1.3329923694090126e-05, 'residents_garden(f)'),\n",
       " (0.0, 'year')]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_features = sorted(zip(fsclf.feature_importances_, names), reverse=True)\n",
    "sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['day',\n",
       " 'price_modify',\n",
       " 'month',\n",
       " 'latitude_modify',\n",
       " 'photos_number',\n",
       " 'longitude_modify',\n",
       " 'bedrooms',\n",
       " 'hardwood_floors(f)',\n",
       " 'apartment(d)',\n",
       " 'of(d)',\n",
       " 'bedroom(d)',\n",
       " 'no_fee(f)',\n",
       " 'on(d)',\n",
       " 'laundry_in_building(f)',\n",
       " 'large(d)',\n",
       " 'kitchen(d)',\n",
       " 'renovated(d)',\n",
       " 'great(d)',\n",
       " 'building(d)',\n",
       " 'call(d)',\n",
       " 'located(d)',\n",
       " 'bathrooms',\n",
       " 'hardwood(d)',\n",
       " 'all(d)',\n",
       " 'doorman(f)',\n",
       " 'high(d)',\n",
       " 'living(d)',\n",
       " 'contact(d)',\n",
       " 'one(d)',\n",
       " 'steel(d)',\n",
       " 'beautiful(d)',\n",
       " 'no(d)',\n",
       " 'has(d)',\n",
       " 'full(d)',\n",
       " 'east(d)',\n",
       " 'elevator(f)',\n",
       " 'spacious(d)',\n",
       " 'laundry(d)',\n",
       " 'stainless(d)',\n",
       " 'email(d)',\n",
       " 'closet(d)',\n",
       " 'room(d)',\n",
       " 'unit(d)',\n",
       " 'studio(d)',\n",
       " 'private(d)',\n",
       " 'close(d)',\n",
       " 'dishwasher(f)',\n",
       " 'marble(d)',\n",
       " 'granite(d)',\n",
       " 'size(d)',\n",
       " 'bathroom(d)',\n",
       " 'space(d)',\n",
       " 'text(d)',\n",
       " 'amazing(d)',\n",
       " 'laundry_in_unit(f)',\n",
       " 'appliances(d)',\n",
       " 'available(d)',\n",
       " 'steps(d)',\n",
       " 'windows(d)',\n",
       " 'two(d)',\n",
       " 'floors(d)',\n",
       " 'subway(d)',\n",
       " 'west(d)',\n",
       " 'schedule(d)',\n",
       " 'heart(d)',\n",
       " 'floor(d)',\n",
       " 'have(d)',\n",
       " 'central(d)',\n",
       " 'cats_allowed(f)',\n",
       " 'dogs_allowed(f)',\n",
       " 'free(d)',\n",
       " 'please(d)',\n",
       " 'access(d)',\n",
       " 'modern(d)',\n",
       " 'open(d)',\n",
       " 'fitness_center(f)',\n",
       " 'including(d)',\n",
       " 'city(d)',\n",
       " 'luxury(d)',\n",
       " 'reduced_fee(f)',\n",
       " 'information(d)',\n",
       " 'doorman(d)',\n",
       " 'broker(d)',\n",
       " 'apartments(d)',\n",
       " 'dining_room(f)',\n",
       " 'fitness(d)',\n",
       " 'real(d)',\n",
       " 'exclusive(f)',\n",
       " 'york(d)',\n",
       " 'outdoor_space(f)',\n",
       " 'roof_deck(f)',\n",
       " 'estate(d)',\n",
       " 'loft(f)',\n",
       " 'balcony(f)',\n",
       " 'high_speed_internet(f)',\n",
       " 'furnished(f)',\n",
       " 'swimming_pool(f)',\n",
       " 'new_construction(f)',\n",
       " 'private_outdoor_space(f)',\n",
       " 'common_outdoor_space(f)',\n",
       " 'terrace(f)',\n",
       " 'equal(d)',\n",
       " 'patio(f)',\n",
       " 'garden(f)',\n",
       " 'housing(d)',\n",
       " 'supports(d)',\n",
       " 'wheelchair_access(f)',\n",
       " 'fireplace(f)',\n",
       " 'prewar(f)',\n",
       " 'parking_space(f)',\n",
       " 'renovated(f)',\n",
       " 'multi(f)',\n",
       " 'level(f)',\n",
       " 'laundry_room(f)',\n",
       " 'high_ceilings(f)',\n",
       " 'garage(f)',\n",
       " 'simplex(f)',\n",
       " 'newly_renovated(f)',\n",
       " 'light(f)',\n",
       " 'pets_on_approval(f)',\n",
       " 'short_term_allowed(f)',\n",
       " 'hardwood(f)',\n",
       " 'site_laundry(f)',\n",
       " 'actual_apt(f)',\n",
       " 'deck(f)',\n",
       " 'high_ceiling(f)',\n",
       " 'stainless_steel_appliances(f)',\n",
       " 'lowrise(f)',\n",
       " 'live_in_super(f)',\n",
       " 'live(f)',\n",
       " '_photos(f)',\n",
       " 'no_pets(f)',\n",
       " 'exposed_brick(f)',\n",
       " 'dryer(f)',\n",
       " 'dryer_in_unit(f)',\n",
       " 'green_building(f)',\n",
       " 'storage(f)',\n",
       " 'in_superintendent(f)',\n",
       " 'marble_bath(f)',\n",
       " 'roof(f)',\n",
       " 'washer(f)',\n",
       " 'site_garage(f)',\n",
       " 'walk_in_closet(f)',\n",
       " 'washer_in_unit(f)',\n",
       " 'laundry(f)',\n",
       " 'publicoutdoor(f)',\n",
       " 'residents_lounge(f)',\n",
       " 'subway(f)',\n",
       " 'pool(f)',\n",
       " 'granite_kitchen(f)']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_selected = [sorted_features[i][1] for i in range(150)]\n",
    "feature_selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=X[feature_selected]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "             estimator=OneVsRestClassifier(estimator=HistGradientBoostingClassifier(l2_regularization=0.0,\n",
       "                                                                                    learning_rate=0.1,\n",
       "                                                                                    loss='auto',\n",
       "                                                                                    max_bins=256,\n",
       "                                                                                    max_depth=None,\n",
       "                                                                                    max_iter=100,\n",
       "                                                                                    max_leaf_nodes=31,\n",
       "                                                                                    min_samples_leaf=20,\n",
       "                                                                                    n_iter_no_change=None,\n",
       "                                                                                    random_state=None,\n",
       "                                                                                    scoring=None,\n",
       "                                                                                    tol=1e-07,\n",
       "                                                                                    validation_fraction=0.1,\n",
       "                                                                                    verbose=0),\n",
       "                                           n_jobs=None),\n",
       "             iid='warn', n_jobs=None,\n",
       "             param_grid={'estimator__max_iter': range(210, 260, 10)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_log_loss', verbose=0)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GridSearchCV to find optimal max_iter, tol\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# specify number of folds for k-fold CV\n",
    "n_folds = 5\n",
    "\n",
    "\n",
    "# instantiate the model (note we are specifying a max_depth)\n",
    "model_to_set = OneVsRestClassifier(HistGradientBoostingClassifier())\n",
    "# parameters to build the model on\n",
    "parameters = {\n",
    "    \"estimator__max_iter\":range(210,260,10),\n",
    "#     \"estimator__max_leaf_nodes\":range(25,40,5),\n",
    "#     \"estimator__tol\":[1e-6,0.5*1e-7,1e-7,0.5*1e-8,1e-8],\n",
    "#     \"estimator__min_samples_leaf\":range(10,25,5)\n",
    "#     \"estimator__max_depth\":[5,6,7]    \n",
    "}\n",
    "# fit tree on training data\n",
    "HGB = GridSearchCV(model_to_set, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"neg_log_loss\")   #neg_log_loss is simply equal to - log_loss\n",
    "HGB.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_estimator__max_iter</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>27.002723</td>\n",
       "      <td>1.528679</td>\n",
       "      <td>0.137892</td>\n",
       "      <td>0.011177</td>\n",
       "      <td>210</td>\n",
       "      <td>{'estimator__max_iter': 210}</td>\n",
       "      <td>-0.567558</td>\n",
       "      <td>-0.576933</td>\n",
       "      <td>-0.587676</td>\n",
       "      <td>-0.599680</td>\n",
       "      <td>-0.596830</td>\n",
       "      <td>-0.585735</td>\n",
       "      <td>0.012077</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>25.529671</td>\n",
       "      <td>0.495937</td>\n",
       "      <td>0.133028</td>\n",
       "      <td>0.023447</td>\n",
       "      <td>220</td>\n",
       "      <td>{'estimator__max_iter': 220}</td>\n",
       "      <td>-0.567105</td>\n",
       "      <td>-0.576243</td>\n",
       "      <td>-0.587271</td>\n",
       "      <td>-0.599493</td>\n",
       "      <td>-0.596717</td>\n",
       "      <td>-0.585365</td>\n",
       "      <td>0.012237</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>25.138426</td>\n",
       "      <td>0.675040</td>\n",
       "      <td>0.129332</td>\n",
       "      <td>0.014783</td>\n",
       "      <td>230</td>\n",
       "      <td>{'estimator__max_iter': 230}</td>\n",
       "      <td>-0.566790</td>\n",
       "      <td>-0.576096</td>\n",
       "      <td>-0.587312</td>\n",
       "      <td>-0.599803</td>\n",
       "      <td>-0.596266</td>\n",
       "      <td>-0.585253</td>\n",
       "      <td>0.012345</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>26.039024</td>\n",
       "      <td>0.174455</td>\n",
       "      <td>0.139787</td>\n",
       "      <td>0.012643</td>\n",
       "      <td>240</td>\n",
       "      <td>{'estimator__max_iter': 240}</td>\n",
       "      <td>-0.566430</td>\n",
       "      <td>-0.575782</td>\n",
       "      <td>-0.587034</td>\n",
       "      <td>-0.599966</td>\n",
       "      <td>-0.596183</td>\n",
       "      <td>-0.585079</td>\n",
       "      <td>0.012514</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>26.550330</td>\n",
       "      <td>0.192151</td>\n",
       "      <td>0.130377</td>\n",
       "      <td>0.005729</td>\n",
       "      <td>250</td>\n",
       "      <td>{'estimator__max_iter': 250}</td>\n",
       "      <td>-0.566456</td>\n",
       "      <td>-0.575458</td>\n",
       "      <td>-0.587269</td>\n",
       "      <td>-0.600047</td>\n",
       "      <td>-0.596028</td>\n",
       "      <td>-0.585051</td>\n",
       "      <td>0.012555</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      27.002723      1.528679         0.137892        0.011177   \n",
       "1      25.529671      0.495937         0.133028        0.023447   \n",
       "2      25.138426      0.675040         0.129332        0.014783   \n",
       "3      26.039024      0.174455         0.139787        0.012643   \n",
       "4      26.550330      0.192151         0.130377        0.005729   \n",
       "\n",
       "  param_estimator__max_iter                        params  split0_test_score  \\\n",
       "0                       210  {'estimator__max_iter': 210}          -0.567558   \n",
       "1                       220  {'estimator__max_iter': 220}          -0.567105   \n",
       "2                       230  {'estimator__max_iter': 230}          -0.566790   \n",
       "3                       240  {'estimator__max_iter': 240}          -0.566430   \n",
       "4                       250  {'estimator__max_iter': 250}          -0.566456   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0          -0.576933          -0.587676          -0.599680          -0.596830   \n",
       "1          -0.576243          -0.587271          -0.599493          -0.596717   \n",
       "2          -0.576096          -0.587312          -0.599803          -0.596266   \n",
       "3          -0.575782          -0.587034          -0.599966          -0.596183   \n",
       "4          -0.575458          -0.587269          -0.600047          -0.596028   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0        -0.585735        0.012077                5  \n",
       "1        -0.585365        0.012237                4  \n",
       "2        -0.585253        0.012345                3  \n",
       "3        -0.585079        0.012514                2  \n",
       "4        -0.585051        0.012555                1  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# scores of GridSearch CV\n",
    "scores = HGB.cv_results_\n",
    "pd.DataFrame(scores).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HGB.best_params_['estimator__max_iter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1e-06"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"estimator__tol\":[1e-6,1e-7,1e-8]\n",
    "}\n",
    "\n",
    "# fit tree on training data\n",
    "HGB = GridSearchCV(model_to_set, parameters, \n",
    "                    cv=n_folds, \n",
    "                   scoring=\"neg_log_loss\")   #neg_log_loss is simply equal to - log_loss\n",
    "# HGB = OneVsRestClassifier(HGB)\n",
    "HGB.fit(X, Y)\n",
    "HGB.best_params_['estimator__tol']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training log_loss: 0.436\n",
      "validation log_loss: 0.583\n",
      "[[[4509   54]\n",
      "  [ 279   94]]\n",
      "\n",
      " [[ 884  622]\n",
      "  [ 354 3076]]\n",
      "\n",
      " [[3651  152]\n",
      "  [ 970  163]]]\n",
      "training log_loss: 0.433\n",
      "validation log_loss: 0.602\n",
      "[[[4439   61]\n",
      "  [ 334  102]]\n",
      "\n",
      " [[ 911  659]\n",
      "  [ 356 3010]]\n",
      "\n",
      " [[3632  170]\n",
      "  [ 959  175]]]\n",
      "training log_loss: 0.437\n",
      "validation log_loss: 0.566\n",
      "[[[4520   45]\n",
      "  [ 290   80]]\n",
      "\n",
      " [[ 852  612]\n",
      "  [ 333 3138]]\n",
      "\n",
      " [[3694  147]\n",
      "  [ 937  157]]]\n",
      "training log_loss: 0.434\n",
      "validation log_loss: 0.588\n",
      "[[[4478   44]\n",
      "  [ 312  101]]\n",
      "\n",
      " [[ 876  659]\n",
      "  [ 380 3020]]\n",
      "\n",
      " [[3650  163]\n",
      "  [ 974  148]]]\n",
      "training log_loss: 0.437\n",
      "validation log_loss: 0.562\n",
      "[[[4543   48]\n",
      "  [ 268   76]]\n",
      "\n",
      " [[ 826  623]\n",
      "  [ 363 3123]]\n",
      "\n",
      " [[3689  141]\n",
      "  [ 958  147]]]\n",
      "training log_loss: 0.437\n",
      "validation log_loss: 0.590\n",
      "[[[4489   43]\n",
      "  [ 315   88]]\n",
      "\n",
      " [[ 864  630]\n",
      "  [ 343 3098]]\n",
      "\n",
      " [[3697  147]\n",
      "  [ 945  146]]]\n",
      "training log_loss: 0.436\n",
      "validation log_loss: 0.581\n",
      "[[[4544   47]\n",
      "  [ 268   76]]\n",
      "\n",
      " [[ 862  662]\n",
      "  [ 357 3054]]\n",
      "\n",
      " [[3596  159]\n",
      "  [1015  165]]]\n",
      "training log_loss: 0.437\n",
      "validation log_loss: 0.575\n",
      "[[[4507   48]\n",
      "  [ 301   79]]\n",
      "\n",
      " [[ 857  630]\n",
      "  [ 346 3102]]\n",
      "\n",
      " [[3674  154]\n",
      "  [ 961  146]]]\n",
      "training log_loss: 0.435\n",
      "validation log_loss: 0.596\n",
      "[[[4497   46]\n",
      "  [ 315   77]]\n",
      "\n",
      " [[ 875  693]\n",
      "  [ 335 3032]]\n",
      "\n",
      " [[3606  153]\n",
      "  [1024  152]]]\n",
      "training log_loss: 0.437\n",
      "validation log_loss: 0.571\n",
      "average validation log_loss: 0.581\n",
      "[[[4512   39]\n",
      "  [ 297   87]]\n",
      "\n",
      " [[ 840  631]\n",
      "  [ 369 3095]]\n",
      "\n",
      " [[3703  145]\n",
      "  [ 926  161]]]\n",
      "One vs rest best validation accuracy: 0.562\n",
      "[0.6614654753648804, 0.6511334248049866, 0.6644157894736842, 0.66443706878243, 0.6523363870087472, 0.6670192747467295, 0.6496656304321685, 0.6466345404573625, 0.6461122158146435, 0.6824231571318812]\n",
      "[0.4308898565570036, 0.427500808084846, 0.4212628336552499, 0.4215648868689746, 0.41661036593873274, 0.41750137962550377, 0.41869978380363465, 0.4131448981821333, 0.408728390871248, 0.42271744200478323]\n",
      "[0.6685575364667747, 0.6559967585089141, 0.6749746707193516, 0.6536980749746707, 0.670516717325228, 0.6666666666666666, 0.6603850050658562, 0.6662613981762918, 0.6555217831813577, 0.6690982776089159]\n"
     ]
    }
   ],
   "source": [
    "training_score = 0\n",
    "validation_score = 0\n",
    "Best_validation_score = 20\n",
    "count = 0\n",
    "total_validation_score = 0\n",
    "precision = []\n",
    "recall = []\n",
    "test_accuracy =[]\n",
    "test_accuracy_value = 0\n",
    "test_precision = 0\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "kf = RepeatedKFold(n_splits=10, n_repeats=1, random_state=None) \n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X):\n",
    "#     print(\"Train:\", train_index, \"Validation:\",test_index)\n",
    "    X['id']=list(range(0,49352))\n",
    "    X.set_index(\"id\", inplace=True)\n",
    "    Y['id']=list(range(0,49352))\n",
    "    Y.set_index(\"id\", inplace=True)\n",
    "    X_train, X_test = X.loc[train_index,:], X.loc[test_index,:]\n",
    "    Y_train, Y_test = Y.loc[train_index,:], Y.loc[test_index,:]\n",
    "    ABC = OneVsRestClassifier(HistGradientBoostingClassifier(max_iter=250, tol=1e-08)) #max_iter=200 300not good\n",
    "    ABC = ABC.fit(X_train,Y_train)\n",
    "#   classification accuracy metrics\n",
    "    if test_accuracy_value<ABC.score(X_test,Y_test):\n",
    "        test_accuracy_value=ABC.score(X_test,Y_test)\n",
    "        Best_ABC2 = ABC\n",
    "    test_accuracy.append(ABC.score(X_test,Y_test))\n",
    "#   log loss on training dataset\n",
    "    Y_Prediction=ABC.predict_proba(X_train)\n",
    "    Y_Prediction=pd.DataFrame(Y_Prediction)\n",
    "    Y_train=pd.DataFrame(Y_train)\n",
    "    training_score=log_loss(Y_train, Y_Prediction)\n",
    "    print(\"training log_loss: %0.3f\" % training_score)\n",
    "\n",
    "#   log loss on test dataset\n",
    "    Y_Prediction=ABC.predict_proba(X_test)\n",
    "    Y_Prediction=pd.DataFrame(Y_Prediction)\n",
    "    Y_test=pd.DataFrame(Y_test)\n",
    "    validation_score=log_loss(Y_test, Y_Prediction)\n",
    "    count+=1\n",
    "    total_validation_score+=validation_score\n",
    "    print(\"validation log_loss: %0.3f\" % validation_score)\n",
    "\n",
    "#   logloss metrics\n",
    "    if validation_score<Best_validation_score:\n",
    "        Best_validation_score = validation_score\n",
    "        Best_ABC1 = ABC\n",
    "\n",
    "    if count==10:\n",
    "        average = total_validation_score/float(count)\n",
    "        print(\"average validation log_loss: %0.3f\" % average)\n",
    "        count=0\n",
    "        total_validation_score=0\n",
    "    \n",
    "    Y_Prediction=ABC.predict(X_test)\n",
    "#   print confusion matrix\n",
    "    print(multilabel_confusion_matrix(Y_test, Y_Prediction))\n",
    "#   confusion matrix matrics\n",
    "    if test_precision<precision_score(Y_test, Y_Prediction, average='macro'):\n",
    "        test_precision=precision_score(Y_test, Y_Prediction, average='macro')\n",
    "        Best_ABC3 = ABC\n",
    "    precision.append(precision_score(Y_test, Y_Prediction, average='macro'))\n",
    "    recall.append(recall_score(Y_test, Y_Prediction, average='macro'))\n",
    "print (\"One vs rest best validation accuracy: %0.3f\" % Best_validation_score) \n",
    "print(precision)\n",
    "print(recall)\n",
    "print (test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_Test=X_Test[feature_selected]\n",
    "Y_Prediction=Best_ABC1.predict_proba(X_Test)  #use the best classifier evaluated by log loss\n",
    "y=pd.DataFrame(Y_Prediction)\n",
    "y.reset_index(drop=True, inplace=True)\n",
    "ids = test['listing_id']\n",
    "ids.reset_index(drop=True, inplace=True)\n",
    "out = pd.concat([ids, y],axis=1, ignore_index=True)\n",
    "out\n",
    "# out.columns=['listing_id','high','medium','low']\n",
    "out.columns=['listing_id','high','low','medium']\n",
    "submission_file = open(\"submission.csv\", \"w\")\n",
    "out.to_csv('submission.csv', index = False)\n",
    "submission_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
